{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/aashaiavadhani/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/aashaiavadhani/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/aashaiavadhani/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/aashaiavadhani/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/aashaiavadhani/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/aashaiavadhani/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/aashaiavadhani/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/aashaiavadhani/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/aashaiavadhani/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/aashaiavadhani/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/aashaiavadhani/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/aashaiavadhani/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#BERT MODEL CLASSIFICATION\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torchnlp.datasets import imdb_dataset\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "import tqdm\n",
    "from tqdm import tqdm, trange\n",
    "#WORK WITH TITLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frameRealOnion = pd.read_csv('real_onion_good.csv')\n",
    "data_frameRealOnion = data_frameRealOnion[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fakeOnion = pd.read_csv('better_fake_onion.csv')\n",
    "data_fakeOnion = data_fakeOnion[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fakeOnion.dropna(inplace = True)\n",
    "data_frameRealOnion.dropna(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HAVE TO CREATE A DATAFRAME THAT HAS A COMBINATION OF BOTH DATA\n",
    "frames = [data_frameRealOnion,data_fakeOnion]\n",
    "\n",
    "#data_fakeOnion.head()\n",
    "bigDataOnion = pd.concat(frames)\n",
    "bigDataOnion = bigDataOnion.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#70%: Train, 30%: Test\n",
    "data_length = len(bigDataOnion)\n",
    "train_data_text = bigDataOnion[:80]\n",
    "test_data_text = bigDataOnion[41:81]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZING\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['programmer','homemaker']\n",
    "\n",
    "\n",
    "\"This code gathers and returns the word embeddings for all the most similar words for the key phrases\"\n",
    "def getClusters(keys):\n",
    "    embedCluster = []\n",
    "    wordCluster = []\n",
    "    for word in keys:\n",
    "        embeddings,words = [], []\n",
    "        modelSimilarWords = model.most_similar(word, topn=40)\n",
    "        \n",
    "#Need to iterate through the word and its embedding, thus temp var is needed\n",
    "        for similar_word, temp in modelSimilarWords:\n",
    "            words.append(similar_word)\n",
    "            embeddings.append(model[similar_word])\n",
    "            \n",
    "        embedCluster.append(embeddings)\n",
    "        wordCluster.append(words)\n",
    "        \n",
    "    return (embedCluster,wordCluster)\n",
    "\n",
    "\n",
    "embedding = getClusters(keys)[0]\n",
    "wordClustering = getClusters(keys)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the clustering for the T-SNE\n",
    "\n",
    "embedding_clusters = np.array(embedding)\n",
    "n, m, k = embedding_clusters.shape\n",
    "\n",
    "#Creating the TSNE model\n",
    "#n iter allows for optimization of the model\n",
    "#the random state produces the random number if the case for some values is None\n",
    "tsne_model_en_2d = TSNE(perplexity=30, n_components=2, init = 'pca' ,n_iter=4000, random_state=35)\n",
    "\n",
    "#transformes the embedding cluster model to 2 dimensions\n",
    "transform2DEmbedding = tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))\n",
    "\n",
    "#creating an array in order to \n",
    "array2DEmbedding = np.array(transform2DEmbedding)\n",
    "finalEmbedding2D = array2DEmbedding.reshape(n, m, 2)\n",
    "\n",
    "\n",
    "\n",
    "def plotTSNE(title, labels, embedding_clusters, word_clusters, a, filename=None):\n",
    "    \n",
    "    #creating the plot\n",
    "    plt.figure(figsize=(17, 10))\n",
    "    num_labels = len(labels)\n",
    "    color = cm.rainbow(np.linspace(0, 1, num_labels))\n",
    "    zippedData = zip(labels, embedding_clusters, word_clusters, color)\n",
    "    for label, embeddings, words, color in zippedData:\n",
    "        x, y = embeddings[:, 0], embeddings[:, 1]\n",
    "        plt.scatter(x, y, c=color, alpha=a, label=label)\n",
    "        \n",
    "        #getting the word neighbors\n",
    "        wordEnumeration = enumerate(words)            \n",
    "        #labeling the embedding coordinates with their corresponding word\n",
    "        for locationEmbedding, word in wordEnumeration:\n",
    "            plt.annotate(word, alpha=0.5, xy=(x[locationEmbedding], y[locationEmbedding]), xytext=(5, 2),\n",
    "                         textcoords='offset points', ha='right', va='bottom', size=10)\n",
    "\n",
    "    plt.title(title), plt.legend(loc = 4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(train_data_text))\n",
    "#print(train_data_text['label'])\n",
    "#print(list(train_data_text['label']))\n",
    "#print(list(train_data_text['title']))\n",
    "\n",
    "train_data = []\n",
    "for index_label in range(len(list(train_data_text['label']))):\n",
    "    label = list(train_data_text['label'])[index_label]\n",
    "    text = list(train_data_text['title'])[index_label]\n",
    "    dictionary = {'title': text, 'label': label}\n",
    "    train_data.append(dictionary)\n",
    "\n",
    "\n",
    "\n",
    "test_data = []\n",
    "for index_label in range(len(list(test_data_text['label']))):\n",
    "    label = list(test_data_text['label'])[index_label]\n",
    "    text = list(test_data_text['title'])[index_label]\n",
    "    dictionary = {'title': text, 'label': label}\n",
    "    test_data.append(dictionary)\n",
    "\n",
    "\n",
    "#train_data = [{'title': text, 'label': type_data } for text in list(train_data_text['title']) for type_data in list(train_data_text['label'])]\n",
    "#test_data = [{'title': text, 'label': type_data } for text in list(test_data_text['title']) for type_data in list(test_data_text['label'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels = list(zip(*map(lambda bigDataOnion: (bigDataOnion['title'], bigDataOnion['label']), train_data)))\n",
    "test_texts, test_labels = list(zip(*map(lambda bigDataOnion: (bigDataOnion['title'], bigDataOnion['label']), test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts))\n",
    "print(len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], train_texts))\n",
    "print(\"hello\")\n",
    "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens))\n",
    "test_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, test_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_ids = pad_sequences(train_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "test_tokens_ids = pad_sequences(test_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False False False False  True False  True False False  True\n",
      " False False  True False  True False  True False False  True  True False\n",
      "  True False False False  True False False  True False  True False False\n",
      " False  True False False False False False False False False  True False\n",
      " False  True  True False  True False  True  True False False False  True\n",
      " False False False  True False False  True False  True  True  True False\n",
      " False False  True  True False  True  True False]\n"
     ]
    }
   ],
   "source": [
    "train_y = np.array(train_labels) == 0\n",
    "test_y = np.array(test_labels) == 0\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertBinaryClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokens, masks=None):\n",
    "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizing each of the text data through the tensor masks\n",
    "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
    "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]\n",
    "train_masks_tensor = torch.tensor(train_masks)\n",
    "test_masks_tensor = torch.tensor(test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the trained tokens into torch tensors \n",
    "\n",
    "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
    "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
    "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
    "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the BERT Classifier into training dataset, sampler, and the loader\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 1\n",
    "train_dataset =  torch.utils.data.TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
    "train_sampler =  torch.utils.data.RandomSampler(train_dataset)\n",
    "train_dataloader =  torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "test_dataset =  torch.utils.data.TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
    "test_sampler =  torch.utils.data.SequentialSampler(test_dataset)\n",
    "test_dataloader =  torch.utils.data.DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "0/80.0 loss: 0.6663848161697388 \n",
      "Epoch:  1\n",
      "1/80.0 loss: 0.660313069820404 \n",
      "Epoch:  1\n",
      "2/80.0 loss: 0.7485995292663574 \n",
      "Epoch:  1\n",
      "3/80.0 loss: 0.7560811638832092 \n",
      "Epoch:  1\n",
      "4/80.0 loss: 0.7052189946174622 \n",
      "Epoch:  1\n",
      "5/80.0 loss: 0.6896471480528513 \n",
      "Epoch:  1\n",
      "6/80.0 loss: 0.7013253484453473 \n",
      "Epoch:  1\n",
      "7/80.0 loss: 0.724766306579113 \n",
      "Epoch:  1\n",
      "8/80.0 loss: 0.717884447839525 \n",
      "Epoch:  1\n",
      "9/80.0 loss: 0.729378092288971 \n",
      "Epoch:  1\n",
      "10/80.0 loss: 0.7271339784968983 \n",
      "Epoch:  1\n",
      "11/80.0 loss: 0.7209624797105789 \n",
      "Epoch:  1\n",
      "12/80.0 loss: 0.719561925301185 \n",
      "Epoch:  1\n",
      "13/80.0 loss: 0.7281773643834251 \n",
      "Epoch:  1\n",
      "14/80.0 loss: 0.7321641405423482 \n",
      "Epoch:  1\n",
      "15/80.0 loss: 0.7302170656621456 \n",
      "Epoch:  1\n",
      "16/80.0 loss: 0.7221196609384873 \n",
      "Epoch:  1\n",
      "17/80.0 loss: 0.7171893053584628 \n",
      "Epoch:  1\n",
      "18/80.0 loss: 0.7130195686691686 \n",
      "Epoch:  1\n",
      "19/80.0 loss: 0.7050286084413528 \n",
      "Epoch:  1\n",
      "20/80.0 loss: 0.6972067412875947 \n",
      "Epoch:  1\n",
      "21/80.0 loss: 0.6934465305371718 \n",
      "Epoch:  1\n",
      "22/80.0 loss: 0.6927718986635623 \n",
      "Epoch:  1\n",
      "23/80.0 loss: 0.6913666725158691 \n",
      "Epoch:  1\n",
      "24/80.0 loss: 0.6903898048400879 \n",
      "Epoch:  1\n",
      "25/80.0 loss: 0.6866553746736966 \n",
      "Epoch:  1\n",
      "26/80.0 loss: 0.6848286875972042 \n",
      "Epoch:  1\n",
      "27/80.0 loss: 0.6803976744413376 \n",
      "Epoch:  1\n",
      "28/80.0 loss: 0.6839259205193355 \n",
      "Epoch:  1\n",
      "29/80.0 loss: 0.6879611988862355 \n",
      "Epoch:  1\n",
      "30/80.0 loss: 0.6953753521365504 \n",
      "Epoch:  1\n",
      "31/80.0 loss: 0.694795448333025 \n",
      "Epoch:  1\n",
      "32/80.0 loss: 0.6921504844318737 \n",
      "Epoch:  1\n",
      "33/80.0 loss: 0.6913689392454484 \n",
      "Epoch:  1\n",
      "34/80.0 loss: 0.6936765466417585 \n",
      "Epoch:  1\n",
      "35/80.0 loss: 0.6937130507495668 \n",
      "Epoch:  1\n",
      "36/80.0 loss: 0.6936697412181545 \n",
      "Epoch:  1\n",
      "37/80.0 loss: 0.6904727847952592 \n",
      "Epoch:  1\n",
      "38/80.0 loss: 0.6929115698887751 \n",
      "Epoch:  1\n",
      "39/80.0 loss: 0.6892967104911805 \n",
      "Epoch:  1\n",
      "40/80.0 loss: 0.6871947515301589 \n",
      "Epoch:  1\n",
      "41/80.0 loss: 0.6897062488964626 \n",
      "Epoch:  1\n",
      "42/80.0 loss: 0.6902031163836635 \n",
      "Epoch:  1\n",
      "43/80.0 loss: 0.6893221695314754 \n",
      "Epoch:  1\n",
      "44/80.0 loss: 0.6904910021358066 \n",
      "Epoch:  1\n",
      "45/80.0 loss: 0.6875285752441572 \n",
      "Epoch:  1\n",
      "46/80.0 loss: 0.6845031900608793 \n",
      "Epoch:  1\n",
      "47/80.0 loss: 0.6854596870640913 \n",
      "Epoch:  1\n",
      "48/80.0 loss: 0.6825975301314373 \n",
      "Epoch:  1\n",
      "49/80.0 loss: 0.6814373672008515 \n",
      "Epoch:  1\n",
      "50/80.0 loss: 0.6769489038224313 \n",
      "Epoch:  1\n",
      "51/80.0 loss: 0.6783132381164111 \n",
      "Epoch:  1\n",
      "52/80.0 loss: 0.6764582969107718 \n",
      "Epoch:  1\n",
      "53/80.0 loss: 0.6767232605704555 \n",
      "Epoch:  1\n",
      "54/80.0 loss: 0.6782713554122232 \n",
      "Epoch:  1\n",
      "55/80.0 loss: 0.6778358252985137 \n",
      "Epoch:  1\n",
      "56/80.0 loss: 0.6768655766520584 \n",
      "Epoch:  1\n",
      "57/80.0 loss: 0.6811812499473835 \n",
      "Epoch:  1\n",
      "58/80.0 loss: 0.6839003179032924 \n",
      "Epoch:  1\n",
      "59/80.0 loss: 0.6830368389685949 \n",
      "Epoch:  1\n",
      "60/80.0 loss: 0.6861708848202814 \n",
      "Epoch:  1\n",
      "61/80.0 loss: 0.6828171005172115 \n",
      "Epoch:  1\n",
      "62/80.0 loss: 0.6801326047806513 \n",
      "Epoch:  1\n",
      "63/80.0 loss: 0.6781307365745306 \n",
      "Epoch:  1\n",
      "64/80.0 loss: 0.6761196063115047 \n",
      "Epoch:  1\n",
      "65/80.0 loss: 0.6772022689833785 \n",
      "Epoch:  1\n",
      "66/80.0 loss: 0.6773815733283314 \n",
      "Epoch:  1\n",
      "67/80.0 loss: 0.6815730482339859 \n",
      "Epoch:  1\n",
      "68/80.0 loss: 0.6802788471830064 \n",
      "Epoch:  1\n",
      "69/80.0 loss: 0.6805897159235818 \n",
      "Epoch:  1\n",
      "70/80.0 loss: 0.6794035812498818 \n",
      "Epoch:  1\n",
      "71/80.0 loss: 0.6757511848376857 \n",
      "Epoch:  1\n",
      "72/80.0 loss: 0.6797833667226034 \n",
      "Epoch:  1\n",
      "73/80.0 loss: 0.6785325798633937 \n",
      "Epoch:  1\n",
      "74/80.0 loss: 0.6779593606789907 \n",
      "Epoch:  1\n",
      "75/80.0 loss: 0.675394677802136 \n",
      "Epoch:  1\n",
      "76/80.0 loss: 0.6724753097280279 \n",
      "Epoch:  1\n",
      "77/80.0 loss: 0.6711658151485981 \n",
      "Epoch:  1\n",
      "78/80.0 loss: 0.6679210602482663 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [04:13<00:00, 253.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "79/80.0 loss: 0.666093573719263 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#The Actual Classifier within BERT as well as the training loop that uses a BCE loss as well as an Adam optimizer. It Implements the backward propation as well as a BCE loss to determine the error of the neural network error\n",
    "\n",
    "bert_clf = BertBinaryClassifier()\n",
    "optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)\n",
    "for epoch_num in tqdm(range(EPOCHS)):\n",
    "    bert_clf.train()\n",
    "    train_loss = 0\n",
    "    for step_num, batch_data in enumerate(train_dataloader):\n",
    "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "        probas = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        batch_loss = loss_func(probas, labels)\n",
    "        train_loss += batch_loss.item()\n",
    "        bert_clf.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch: ', epoch_num + 1)\n",
    "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.12      0.21        17\n",
      "        True       0.61      1.00      0.75        23\n",
      "\n",
      "    accuracy                           0.62        40\n",
      "   macro avg       0.80      0.56      0.48        40\n",
      "weighted avg       0.77      0.62      0.52        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using the BERT classifier on the test data\n",
    "\n",
    "bert_clf.eval()\n",
    "bert_predicted = []\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(test_dataloader):\n",
    "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "        logits = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        loss = loss_func(logits, labels)\n",
    "        numpy_logits = logits.cpu().detach().numpy()\n",
    "        \n",
    "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
    "        all_logits += list(numpy_logits[:, 0])\n",
    "        \n",
    "print(classification_report(test_y, bert_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was only conducted on 40 articles with a training "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
